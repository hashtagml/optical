<div align="center">

# Optical
![ci build](https://github.com/hashtagml/optical/actions/workflows/build.yaml/badge.svg)
[![codecov](https://codecov.io/gh/hashtagml/optical/branch/main/graph/badge.svg?token=EC26I5HFQH)](https://codecov.io/gh/hashtagml/optical)
![Python Version](https://img.shields.io/pypi/pyversions/optical)
[![Documentation Status](https://readthedocs.org/projects/optical/badge/?version=latest)](https://optical.readthedocs.io/en/latest/?badge=latest)
![GitHub all releases](https://img.shields.io/github/downloads/hashtagml/optical/total)
![PyPI - License](https://img.shields.io/pypi/l/optical)
[![PyPI version](https://badge.fury.io/py/optical.svg)](https://badge.fury.io/py/optical)
<!-- [![All Contributors](https://img.shields.io/badge/all_contributors-3-orange.svg?style=flat)](#contributors-) -->

<p align="center"><img align="centre" src="assets/optical_b.png" alt="logo" width = "650"></p>

A collection of utilities for ML vision related tasks.

</div>

## What is optical?

Object detection is one of the mainstream computer vision tasks. However, when it comes to training an object detection model, there is a variety of formats that one has to deal with for different models e.g. `COCO`, `PASCAL VOC`, `Yolo` and so on. `optical` provides a simple interface to convert back and forth between these annotation formats and also perform a bunch of exploratory data analysis (EDA) on these datasets regardless of their source format.

:star2: At present we support the following formats:
- [COCO](https://cocodataset.org/#format-data)
- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#data)
- [Yolo](https://github.com/AlexeyAB/darknet)
- [TFrecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)
- [SageMaker Manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html)
- CSV


## Installation

`optical` could be installed from `pip`:

```sh
pip install optical
```

For conversion to (or from) `TFrecord`, please install the `tensorflow` extra:
```sh
pip install `optical[tensorflow]`
```

for visualisation of images in [mediapy](https://github.com/google/mediapy) format, you need to have [ffmpeg](https://ffmpeg.org/download.html) installed in your system.


## Getting Started

### declare the imports
```python
from optical import Annotation
```

### read the annotations
```python
annotation = Annotation(root = "/path/to/dataset", format="coco")
```

optical expects the data to be organised in either of the following layouts:

```sh
root
â”œâ”€â”€ images
â”‚ â”œâ”€â”€ train
â”‚ â”œâ”€â”€ val
â”‚ â””â”€â”€ test
â””â”€â”€ annotations
  â”œâ”€â”€ train.json
  â”œâ”€â”€ val.json
  â””â”€â”€ test.json
```

Note that for annotation formats which require individual annotations for each images (e.g., `PASCAL VOC` or `Yolo`), 
the `annotations` directory should also contain the same sub-directories as in `images`. The splits that do not have an annotation will be ignored.

If your data does not have a split to begin with, that's acceptable too. In that case the directory layout should be like below:

```sh
root
â”œâ”€â”€ images
â”‚ â”œâ”€â”€ 1.jpg
â”‚ â”œâ”€â”€ 2.jpg
â”‚ â”œâ”€â”€ ...
â”‚ â”‚
â”‚ â””â”€â”€ 100.jpg
â”‚
â””â”€â”€ annotations
  â””â”€â”€ label.json
```

Tha name of the annotation file is not important in this case. But, if your format requires individual formats, the annotation files must have the identical name with that of the image.

### EDA
#### Check data distribution

```python
>>> annotation.describe()

| split | images | annotations | categories |
|-------|--------|-------------|------------|
| train | 729    | 1121        | 3          |
| valid | 250    | 322         | 3          |

```
#### Plot label distribution

```python
>>> annotation.show_distribition()
```
<p align="left"><img align="centre" src="assets/show_dist.png" alt="logo" width = "300"></p>


#### Scatter bounding box width and height

```python
>>> annotation.bbox_scatter()
```
<p align="left"><img align="centre" src="assets/bbox_scatter.png" alt="logo" width = "500"></p>

### Visualize images
```python
>>> vis = annotation.visualizer(img_size=256)
>>> vis.show_batch()
```

<p align="left"><img align="centre" src="assets/vis_batch.png" alt="logo" width = "500"></p>

### Split the data if required
```python
>>> splits = annotation.train_test_split(test_size = 0.2, stratified = True)
>>> splits.save("/path/to/output/dir")
```

#### Export to other formats
```python
>>> annotation.export(to = "yolo")
```

## Contributing

### Work in local environment:

1. Fork the repo
2. install poetry:
    ```sh
    curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -
    ```

3. work on virtual environment:
   ```sh
   conda create -n optical python=3.8 pip
   ```

4. install the dependencies and the project in editable mode
   ```sh
   poetry install
   ```
5. Make your changes as required. Please use appropriate use of docstrings (we follow [Google style docstring](https://google.github.io/styleguide/pyguide.html)) and try to keep your code clean.

6. Raise a pull request.

### Work inside the dev container:
If you are a Visual Studio Code user, you may choose to develop inside a container. The benefit is the container comes with all necessary settings and dependencies configured. You will need [Docker](https://www.docker.com/) installed in your system. You also need to have the [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension enabled.

1. Open the project in Visual Studio Code. in the status bar, select open in remote container.

It will perhaps take a few minutes the first time you build the container.

## Contributors âœ¨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align="center"><a href="https://github.com/bishwarup307"><img src="https://avatars.githubusercontent.com/u/15094759?v=4?s=100" width="100px;" alt=""/><br /><sub><b>Bishwarup Bhattacharjee</b></sub></a><br /><a href="https://github.com/hashtagml/optical/commits?author=bishwarup307" title="Code">ðŸ’»</a></td>
    <td align="center"><a href="https://www.linkedin.com/in/satheesh-katipomu/"><img src="https://avatars.githubusercontent.com/u/44122848?v=4?s=100" width="100px;" alt=""/><br /><sub><b>Satheesh K</b></sub></a><br /><a href="https://github.com/hashtagml/optical/commits?author=satheeshkatipomu" title="Code">ðŸ’»</a></td>
    <td align="center"><a href="https://github.com/bharatkumarreddy"><img src="https://avatars.githubusercontent.com/u/6673708?v=4?s=100" width="100px;" alt=""/><br /><sub><b>Bharat Kumar Reddy</b></sub></a><br /><a href="https://github.com/hashtagml/optical/commits?author=bharatkumarreddy" title="Code">ðŸ’»</a></td>
    <td align="center"><a href="https://github.com/aman0044"><img src="https://avatars.githubusercontent.com/u/8287484?v=4?s=100" width="100px;" alt=""/><br /><sub><b>Aman Gupta</b></sub></a><br /><a href="https://github.com/hashtagml/optical/commits?author=aman0044" title="Code">ðŸ’»</a></td>
    
    
  </tr>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!
